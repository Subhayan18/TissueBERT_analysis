# âœ… Step 1.4 - Updated Configuration Summary

## ğŸ¯ What Changed

I've updated the scripts so you can:
- **Keep scripts in**: `/home/chattopa/data_storage/TissueBERT_analysis/step_1_data_preparation/step1.4/`
- **Keep data in**: `/home/chattopa/data_storage/MethAtlas_WGBSanalysis/`

## ğŸ“ Complete Directory Structure

```
/home/chattopa/data_storage/
â”‚
â”œâ”€â”€ TissueBERT_analysis/
â”‚   â””â”€â”€ step_1_data_preparation/
â”‚       â””â”€â”€ step1.4/                                    â† SCRIPTS GO HERE
â”‚           â”œâ”€â”€ config.py                               â† UPDATED
â”‚           â”œâ”€â”€ data_loader.py
â”‚           â”œâ”€â”€ data_augmentation.py
â”‚           â”œâ”€â”€ create_training_dataset.py
â”‚           â”œâ”€â”€ run_step1.4.sh                          â† UPDATED
â”‚           â”œâ”€â”€ README.md
â”‚           â”œâ”€â”€ QUICKSTART.md                           â† UPDATED
â”‚           â””â”€â”€ SETUP_INSTRUCTIONS.md                   â† NEW
â”‚
â””â”€â”€ MethAtlas_WGBSanalysis/                             â† DATA STAYS HERE
    â”œâ”€â”€ panel_beta_matrix.tsv                           â† INPUT
    â”œâ”€â”€ synthetic_reads/                                â† INPUT
    â”‚   â”œâ”€â”€ sample_000_Adipocytes.npz
    â”‚   â”œâ”€â”€ sample_001_Adipocytes_2.npz
    â”‚   â””â”€â”€ ... (119 files)
    â”œâ”€â”€ TWIST_panel_sequences_3mers.txt                 â† INPUT
    â”œâ”€â”€ TWIST_blocks.bed                                â† INPUT
    â”‚
    â””â”€â”€ training_dataset/                               â† OUTPUT (CREATED BY SCRIPTS)
        â”œâ”€â”€ all_data/                                   â† 30M .npz files
        â”‚   â”œâ”€â”€ sample_000_Adipocytes_chr1_273383_273777_aug0.npz
        â”‚   â”œâ”€â”€ sample_000_Adipocytes_chr1_273383_273777_aug1.npz
        â”‚   â””â”€â”€ ... (~30.4 million files, ~300 GB)
        â”œâ”€â”€ metadata.csv                                â† Summary file
        â””â”€â”€ logs/                                       â† Execution logs
            â””â”€â”€ training_dataset_TIMESTAMP.log
```

## ğŸ”§ What Was Modified

### 1. config.py
```python
# NEW: Separate script location from data location
SCRIPT_DIR = "/home/chattopa/data_storage/TissueBERT_analysis/step_1_data_preparation/step1.4"
DATA_BASE_DIR = "/home/chattopa/data_storage/MethAtlas_WGBSanalysis"

# All input paths point to DATA_BASE_DIR
BETA_MATRIX_FILE = os.path.join(DATA_BASE_DIR, "panel_beta_matrix.tsv")
SYNTHETIC_READS_DIR = os.path.join(DATA_BASE_DIR, "synthetic_reads")
# ... etc

# All output paths point to DATA_BASE_DIR
OUTPUT_DIR = os.path.join(DATA_BASE_DIR, "training_dataset")
```

### 2. run_step1.4.sh
```bash
# Navigate to script directory (where you keep the scripts)
cd /home/chattopa/data_storage/TissueBERT_analysis/step_1_data_preparation/step1.4

# Run from here (data writes to MethAtlas_WGBSanalysis)
python3 create_training_dataset.py
```

### 3. Documentation Updated
- **SETUP_INSTRUCTIONS.md** (NEW): Step-by-step setup guide
- **QUICKSTART.md**: Updated for new directory structure
- **README.md**: Still has all the comprehensive documentation

## ğŸš€ Quick Setup

### Step 1: Create Script Directory
```bash
mkdir -p /home/chattopa/data_storage/TissueBERT_analysis/step_1_data_preparation/step1.4
cd /home/chattopa/data_storage/TissueBERT_analysis/step_1_data_preparation/step1.4
```

### Step 2: Download and Copy Scripts
Download all files from Claude, then copy them to the script directory:

```bash
# Copy all scripts here
cp /path/to/downloaded/*.py .
cp /path/to/downloaded/*.sh .
cp /path/to/downloaded/*.md .

# Make SLURM script executable
chmod +x run_step1.4.sh
```

### Step 3: Test
```bash
# Run test mode (from script directory)
python3 create_training_dataset.py --test
```

**Output will appear in:**
`/home/chattopa/data_storage/MethAtlas_WGBSanalysis/training_dataset/`

### Step 4: Full Run
```bash
# Edit partition name in run_step1.4.sh first!
sbatch run_step1.4.sh

# Or run interactively:
python3 create_training_dataset.py
```

## ğŸ“Š What Happens During Execution

```
You execute from:
/home/chattopa/data_storage/TissueBERT_analysis/step_1_data_preparation/step1.4/
          â”‚
          â”œâ”€ Reads inputs from:
          â”‚  /home/chattopa/data_storage/MethAtlas_WGBSanalysis/
          â”‚      â”œâ”€â”€ panel_beta_matrix.tsv
          â”‚      â”œâ”€â”€ synthetic_reads/*.npz
          â”‚      â”œâ”€â”€ TWIST_panel_sequences_3mers.txt
          â”‚      â””â”€â”€ TWIST_blocks.bed
          â”‚
          â””â”€ Writes outputs to:
             /home/chattopa/data_storage/MethAtlas_WGBSanalysis/training_dataset/
                 â”œâ”€â”€ all_data/*.npz (30M files)
                 â”œâ”€â”€ metadata.csv
                 â””â”€â”€ logs/*.log
```

## âœ… Benefits of This Structure

1. **Organized Code**: Scripts are separate from data
2. **Clean Workspace**: Step-by-step analysis organized by phase
3. **Unchanging Data**: All data stays in original location
4. **Easy Navigation**: Clear separation of concerns
5. **Version Control**: Can git track just the script directory

## ğŸ“ Files You Received

### Core Scripts (4 files)
1. **config.py** - Configuration (UPDATED with new paths)
2. **data_loader.py** - Data loading
3. **data_augmentation.py** - Augmentation logic
4. **create_training_dataset.py** - Main script

### Execution (1 file)
5. **run_step1.4.sh** - SLURM submission script (UPDATED)

### Documentation (4 files)
6. **SETUP_INSTRUCTIONS.md** - NEW: Detailed setup guide
7. **QUICKSTART.md** - UPDATED: Quick start with new paths
8. **README.md** - Comprehensive documentation
9. **EXECUTION_SUMMARY.md** - Quick reference

**Total: 9 files ready to use!**

## ğŸ¯ Key Points to Remember

1. **Execute from**: `/home/.../TissueBERT_analysis/.../step1.4/`
2. **Data reads from**: `/home/.../MethAtlas_WGBSanalysis/` (input files)
3. **Data writes to**: `/home/.../MethAtlas_WGBSanalysis/training_dataset/` (outputs)
4. **No need to change directories** - everything is configured with absolute paths

## â±ï¸ Expected Results

- **Test mode**: ~30 min, ~10 GB, 1.3M files
- **Full run**: ~4 hours, ~300 GB, 30.4M files

## ğŸ“ Need Help?

1. Read **SETUP_INSTRUCTIONS.md** for detailed setup
2. Read **QUICKSTART.md** for quick commands
3. Read **README.md** for comprehensive documentation
4. Check logs in: `/home/.../MethAtlas_WGBSanalysis/training_dataset/logs/`

---

**You're all set!** The scripts are configured to work from your organized directory structure while keeping data in its original location.
